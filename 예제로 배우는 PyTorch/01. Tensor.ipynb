{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비 운동: NumPy\n",
    "PyTorch를 소개하기 전에, 먼저 NumPy를 사용하여 신경망을 구성해보겠습니다.\n",
    "\n",
    "NumPy는 N차원 배열 객체(Object)를 제공하며, 이러한 배열을 조작하기 위한 다양한 함수(function)들을 제공합니다. NumPy는 과학적 분야의 연산을 위한 포괄적인 프레임워크(Framework)입니다. 이는 연산 그래프(Computational Graph)나 딥러닝, 변화도(Gradient)에 대해서는 알지 못합니다. 하지만 NumPy 연산을 사용하여 순전파 단계와 역전파 단계를 직접 구현함으로써, 2-계층을 갖는 신경망이 무작위의 데이터를 맞추도록 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N은 배치 크기이며, D_in은 입력의 차원입니다.\n",
    "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1000)\n",
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.23109239,  1.70361579, -0.90928176, -1.00762911, -0.92661709,\n",
       "        1.07801655, -1.25767898,  0.51915244,  0.07633151, -0.07432492])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = np.random.randn(D_in,H)\n",
    "w2 = np.random.randn(H, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Graph](./image/graph.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 28170618.312757254\n",
      "20 292016.5347232647\n",
      "40 46208.00302445215\n",
      "60 11170.74271243954\n",
      "80 3244.1207477468442\n",
      "100 1046.8665327515366\n",
      "120 360.1325722421434\n",
      "140 129.250305569227\n",
      "160 47.77505156727679\n",
      "180 18.034056967069713\n",
      "200 6.913807055446805\n",
      "220 2.684144693009184\n",
      "240 1.0523047540013786\n",
      "260 0.4154950264914969\n",
      "280 0.1649966378285707\n",
      "300 0.06583032802847316\n",
      "320 0.026368931165568845\n",
      "340 0.010598301945787134\n",
      "360 0.004272281261172145\n",
      "380 0.0017267137104910993\n",
      "400 0.0006994888578944493\n",
      "420 0.00028396119492610705\n",
      "440 0.00011549570970474848\n",
      "460 4.705833125577116e-05\n",
      "480 1.9204473323467613e-05\n",
      "500 7.849048729776056e-06\n"
     ]
    }
   ],
   "source": [
    "for t in range(501):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h,0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # 손실(loss)를 계산하고 출력합니다.\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t%20 == 0:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    # 위의 그림 참고\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # 가중치를 갱신합니다.\n",
    "    # 0이 되어야 하므로 빼준다\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Tensor\n",
    "\n",
    "NumPy는 훌륭한 프레임워크지만, GPU를 사용하여 수치 연산을 가소고하할 수는 없습니다. 현대의 심층 신경망에서 GPU는 종종 50배 또는 그 이상의 속도향상을 제공하기 때문에, 안타깝게도 NumPy는 현대의 딥러닝에는 충분치 않습니다.\n",
    "\n",
    "이번에는 PyTorch의 기본적인 개념인 Tensor에 대해서 소개하겠습니다. \n",
    "PyTorch Tensor는 기본적으로 NumPy 배열과 동일합니다:  Tensor는 N차원 배열이며, PyTorch는 Tensor 연산을 위한 다양한 함수들을 제공합니다. NumPy 배열과 같이, PyTorch Tensor는 딥러닝이나 연산 그래프, 벼노하도는 알지 못하며, 과학적 분야의 연산을 위한 포괄적인 도구입니다.\n",
    "\n",
    "그러나 NumPy와는 달리, PyTorch Tensor는 GPU를 활용하여 수치 연산을 가속화할 수 있습니다. GPU에서 PyTorch Tensor를 실행하기 위해서는 단지 새로운 자료형으로 변환(Cast)해주기만 하면 됩니다.\n",
    "\n",
    "여기에서는 PyTorch Tensor를 사용하여 2-계층의 신경망이 무작위 데이터를 맞추도록 할 것입니다. 위의 NumPy 예제에서와 같이 신경망의 순전파 단계와 역전파 단계는 직접 구현하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "#device = torch.device(\"cpu\") cpu에서 실행\n",
    "device = torch.device(\"cuda:0\") #GPU에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N은 배치 크기이며, D_in은 입력의 차원입니다.\n",
    "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31005410.0\n",
      "20 252890.9375\n",
      "40 36332.73828125\n",
      "60 8694.28515625\n",
      "80 2583.27734375\n",
      "100 851.67529296875\n",
      "120 297.59918212890625\n",
      "140 107.85873413085938\n",
      "160 40.087921142578125\n",
      "180 15.179826736450195\n",
      "200 5.846525192260742\n",
      "220 2.2889037132263184\n",
      "240 0.9059869647026062\n",
      "260 0.36188793182373047\n",
      "280 0.14571720361709595\n",
      "300 0.05909295380115509\n",
      "320 0.024190491065382957\n",
      "340 0.010052915662527084\n",
      "360 0.004325928632169962\n",
      "380 0.001980292145162821\n",
      "400 0.0009867745684459805\n",
      "420 0.0005311063141562045\n",
      "440 0.000311279552988708\n",
      "460 0.0001974786282517016\n",
      "480 0.00013243900320958346\n",
      "500 9.313758346252143e-05\n"
     ]
    }
   ],
   "source": [
    "for t in range(501):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    # torch.mm(mat1, mat2, out=None) → Tensor : Performs a matrix multiplication of the matrices mat1 and mat2.\n",
    "    h = x.mm(w1)\n",
    "    # torch.clamp(input, min, max, out=None) → Tensor : Clamp all elements in input into the range [ min, max ] and return a resulting tensor    \n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    # 손실(loss)를 계산하고 출력합니다.\n",
    "    # Use torch.Tensor.item() to get a Python number from a tensor containing a single value:\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t%20 ==0:\n",
    "        print(t, loss)\n",
    "    \n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0]=0\n",
    "    # t() : transpose\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # 경사하강법(Gradient Descent)를 사용하여 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
